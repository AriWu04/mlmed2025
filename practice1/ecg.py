# -*- coding: utf-8 -*-
"""ECG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19FiMoakwKnTpkEH69IhUVmmfvursggO_
"""

!pip install tensorflow

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping

import xgboost as xgb

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

from imblearn.over_sampling import SMOTE

from google.colab import drive
drive.mount('/content/drive')

"""### Load datasets"""

file_path_test = "/content/drive/MyDrive/MLinMed/Data/mitbih_test.csv"
df_test = pd.read_csv(file_path_test, header=None)

print(df_test.head())
print(df_test.info())

file_path_train = "/content/drive/MyDrive/MLinMed/Data/mitbih_train.csv"
df_train = pd.read_csv(file_path_train, header=None)

print(df_train.head())
print(df_train.info())

# Visualize classes
data_labels = df_train.iloc[:, -1]
class_counts = data_labels.value_counts().sort_index()

# Plotting
plt.figure(figsize=(12, 7))
ax = sns.barplot(x=class_counts.index, y=class_counts.values, palette="viridis")
for i, v in enumerate(class_counts.values):
    ax.text(i, v + 200, str(v), ha='center', fontsize=12)

plt.xlabel("Classes")
plt.ylabel("Frequency")
plt.title("Frequency of each class in the ECG Heartbeat Dataset")
plt.savefig("class_frequency.svg")
plt.show()

"""**highly imbalanced**"""

X_train = df_train.iloc[:, :-1].values
y_train = df_train.iloc[:, -1].values   # Labels

X_test = df_test.iloc[:, :-1].values
y_test = df_test.iloc[:, -1].values

# Apply SMOTE to balance classes
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

print(f"Resampled dataset shape: {X_train_resampled.shape}, Labels shape: {y_train_resampled.shape}")

classes = np.unique(y_train)
class_samples = {}

# Extract one sample for each class
for cls in classes:
    class_samples[cls] = X_train[y_train == cls][0]  # First occurrence of each class

# Plot ECG signals for each class
plt.figure(figsize=(12, 6))

for cls, signal in class_samples.items():
    plt.plot(signal, label=f"Class {cls}")

plt.xlabel("Time (samples)")
plt.ylabel("ECG Signal Amplitude")
plt.title("Sample ECG Signals for Each Class in the ECG Heartbeat Dataset")
plt.legend()
plt.savefig("SampleECG.svg")
plt.show()

"""### Model

#### XGBoost
"""

# Normalize data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_resampled)
X_test_scaled = scaler.transform(X_test)  # Normalize test data

# Train XGBoost model
xgb_model = xgb.XGBClassifier(objective="multi:softmax", num_class=5, eval_metric="mlogloss", use_label_encoder=False)
eval_set = [(X_train_scaled, y_train_resampled), (X_test_scaled, y_test)]
xgb_model.fit(X_train_scaled, y_train_resampled,eval_set = eval_set,verbose = True)

# Get training history
evals_result = xgb_model.evals_result()

# Predict and evaluate on test dataset
y_pred_xgb = xgb_model.predict(X_test_scaled)
xgb_accuracy = accuracy_score(y_test, y_pred_xgb)

print(f"XGBoost Test Accuracy: {xgb_accuracy * 100:.2f}%")
print("XGBoost Classification Report:")
print(classification_report(y_test, y_pred_xgb))

"""#### CNN"""

from tensorflow.keras.callbacks import EarlyStopping

# Reshape data for CNN
X_train_cnn = X_train_resampled.reshape(X_train_resampled.shape[0], X_train_resampled.shape[1], 1)
X_test_cnn = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

# Convert labels to categorical (one-hot encoding) for CNN
y_train_cnn = to_categorical(y_train_resampled, num_classes=5)
y_test_cnn = to_categorical(y_test, num_classes=5)

# Define CNN Model with Dropout
cnn_model = tf.keras.models.Sequential([
    tf.keras.layers.Conv1D(64, kernel_size=5, activation='relu', input_shape=(X_train_cnn.shape[1], 1)),
    tf.keras.layers.MaxPooling1D(pool_size=2),
    tf.keras.layers.Dropout(0.3),  # Dropout to reduce overfitting

    tf.keras.layers.Conv1D(128, kernel_size=5, activation='relu'),
    tf.keras.layers.MaxPooling1D(pool_size=2),
    tf.keras.layers.Dropout(0.3),  # Dropout to reduce overfitting

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),  # Dropout before the final layer
    tf.keras.layers.Dense(5, activation='softmax')  # 5 classes
])

# Compile the model
cnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
                  loss='categorical_crossentropy',
                  metrics=['accuracy'])

# Define Early Stopping
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the CNN model with Early Stopping
results = cnn_model.fit(X_train_cnn, y_train_cnn,
                        epochs=20, batch_size=32,
                        validation_data=(X_test_cnn, y_test_cnn),
                        callbacks=[early_stop])

# Evaluate CNN
cnn_loss, cnn_accuracy = cnn_model.evaluate(X_test_cnn, y_test_cnn)
print(f"CNN Test Accuracy: {cnn_accuracy * 100:.2f}%")

"""#### Compare"""

y_pred_cnn = np.argmax(cnn_model.predict(X_test_cnn), axis=1)

print("CNN Classification Report:")
print(classification_report(y_test, y_pred_cnn))

print(f"XGBoost Accuracy: {xgb_accuracy * 100:.2f}%")
print(f"CNN Accuracy: {cnn_accuracy * 100:.2f}%")

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# Compute confusion matrices for XGBoost and CNN
conf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)
conf_matrix_cnn = confusion_matrix(y_test, y_pred_cnn)

# Plot confusion matrix for XGBoost
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.heatmap(conf_matrix_xgb, annot=True, fmt="d", cmap="Blues", xticklabels=[0,1,2,3,4], yticklabels=[0,1,2,3,4])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - XGBoost")

# Plot confusion matrix for CNN
plt.subplot(1, 2, 2)
sns.heatmap(conf_matrix_cnn, annot=True, fmt="d", cmap="Greens", xticklabels=[0,1,2,3,4], yticklabels=[0,1,2,3,4])
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix - CNN")

# Show plots
plt.tight_layout()
plt.savefig("comparison.svg")
plt.show()

"""#### Others"""

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
plt.plot(results.history['loss'], label='Train Loss')
plt.plot(results.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('CNN Train and Validation Loss')
plt.legend()
plt.grid(True)
plt.savefig("CNN_Loss.svg")
plt.show()

"""XgBoost

"""

# Plot training and validation loss
plt.figure(figsize=(8, 5))
plt.plot(evals_result['validation_0']['mlogloss'], label='Train Loss')
plt.plot(evals_result['validation_1']['mlogloss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Log Loss')
plt.title('XGBoost Training and Validation Loss')
plt.legend()
plt.grid(True)
plt.savefig("XGBoost_Loss.svg")
plt.show()